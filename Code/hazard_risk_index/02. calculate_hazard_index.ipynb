{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from shapely.geometry import Point\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# Step 1: Load and preprocess the data\n",
    "# Purpose: We are loading census tract data, points of interest (POI), and hazard data for Georgia to calculate hazard risk scores.\n",
    "census_tract = gpd.read_file('/home/ojin/working_space/SIG/01. justice40/01. data/01. raw/01. Justice40/1.0-shapefile-codebook/filtered_usa/filtered_usa.shp')\n",
    "poi = gpd.read_file('/your/shp/to/calculate/hazard/risk/poi.shp')\n",
    "shapefile_path = r'/home/ojin/working_space/SIG/01. justice40/01. data/02. processed/Georgia_hazard_merged(abs).shp'\n",
    "gdf = gpd.read_file(shapefile_path)\n",
    "\n",
    "# Normalize the building loss rate and flood risk scores using MinMaxScaler (scaled between 0-1)\n",
    "scaler = MinMaxScaler()\n",
    "flood_avg_damage = 3.8  # Example average flood damage from USA disaster statistics\n",
    "gdf['BLR_scaled'] = scaler.fit_transform(gdf[['BLR_abs']])  # Building loss rate (BLR) scaling\n",
    "gdf['Fld_scaled'] = scaler.fit_transform(gdf[['Fld_abs']] * flood_avg_damage)  # Flood risk scaling\n",
    "\n",
    "# Fill NaN values with 0\n",
    "gdf[['BLR_scaled', 'Fld_scaled']] = gdf[['BLR_scaled', 'Fld_scaled']].fillna(0)\n",
    "\n",
    "# Calculate the total hazard score by averaging building loss rate and flood risk\n",
    "gdf['total_cost'] = (gdf['BLR_scaled'] + gdf['Fld_scaled']) / 2\n",
    "\n",
    "# Ensure consistent projection (EPSG:3857) for all datasets\n",
    "census_tract = census_tract.to_crs(epsg=3857)\n",
    "poi = poi.to_crs(epsg=3857)\n",
    "gdf = gdf.to_crs(epsg=3857)\n",
    "\n",
    "# Step 2: Calculate hazard risk scores for each POI\n",
    "# Purpose: Use the buffer areas around each POI to calculate area-weighted risk scores based on the intersecting census tracts.\n",
    "radius_A = 1000  # 1km buffer for nearby hazards\n",
    "radius_B = 3000  # 3km buffer for broader hazards\n",
    "poi_buffers_A = poi.buffer(radius_A)\n",
    "poi_buffers_B = poi.buffer(radius_B)\n",
    "\n",
    "# Function to calculate area-weighted hazard risk scores for each POI\n",
    "def calculate_risk_based_score_polygon(poi_buffer, census_tracts, risk_scores_scaled):\n",
    "    intersecting_tracts = census_tracts[census_tracts.intersects(poi_buffer)]\n",
    "    if not intersecting_tracts.empty:\n",
    "        weights = intersecting_tracts.intersection(poi_buffer).area / intersecting_tracts.area\n",
    "        weighted_risk = np.average(risk_scores_scaled[intersecting_tracts.index], weights=weights)\n",
    "        return weighted_risk\n",
    "    return 0  # Return 0 if no intersecting tracts are found\n",
    "\n",
    "# Calculate risk scores for building loss rate (BLR) and flood risk\n",
    "risk_based_scores_A_scaled = Parallel(n_jobs=5)(delayed(calculate_risk_based_score_polygon)(\n",
    "    poi_buffers_A[i], gdf, gdf['BLR_scaled'].to_numpy()) for i in range(len(poi)))\n",
    "\n",
    "risk_based_scores_B_scaled = Parallel(n_jobs=5)(delayed(calculate_risk_based_score_polygon)(\n",
    "    poi_buffers_B[i], gdf, gdf['Fld_scaled'].to_numpy()) for i in range(len(poi)))\n",
    "\n",
    "# Step 3: Assign hazard risk scores to the POI dataset and scale the final scores\n",
    "# Purpose: Aggregate the BLR and flood risk scores, and rescale the final hazard score between 0-1.\n",
    "aggregate_scores_scaled = (np.array(risk_based_scores_A_scaled) + np.array(risk_based_scores_B_scaled)) / 2\n",
    "poi['hazard_risk'] = aggregate_scores_scaled\n",
    "poi['hazard_risk_scaled'] = scaler.fit_transform(poi[['hazard_risk']])\n",
    "\n",
    "# Save the updated POI shapefile with the hazard risk scores\n",
    "output_shapefile_path = '/home/ojin/working_space/SIG/03. Data/01. Processed/02. filtered/poi_filtered/osm_poi_filterd_with_hazard_scaled.shp'\n",
    "poi.to_file(output_shapefile_path)\n",
    "print(f\"POIs with hazard risk scores saved to: {output_shapefile_path}\")\n",
    "\n",
    "# Step 4: Visualization of hazard risk scores\n",
    "# Purpose: Visualize the POIs with color intensity based on their hazard risk score.\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "poi.plot(column='hazard_risk_scaled', cmap='OrRd', markersize=10, legend=True, ax=ax, edgecolor='black')\n",
    "\n",
    "# Add title and labels to the plot\n",
    "plt.title(\"POIs with Hazard Risk Scores (Scaled)\", fontsize=15)\n",
    "plt.xlabel(\"Longitude\")\n",
    "plt.ylabel(\"Latitude\")\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
